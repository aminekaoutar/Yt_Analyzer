# -*- coding: utf-8 -*-
"""Youtube.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VJCqdBrnOhS_I09TxCBzv59bQ2iSTZk6
"""

pip install youtube-transcript-api

import googleapiclient.discovery
import pandas as pd
from youtube_transcript_api import YouTubeTranscriptApi
from youtube_transcript_api._errors import NoTranscriptFound, TranscriptsDisabled

api_service_name = "youtube"
api_version = "v3"
DEVELOPER_KEY = 'AIzaSyBh55PS-oadpCMl_myJ0IO2mrx-HZtD3JA'
video_id = "7ntHBqTGHVc"

youtube = googleapiclient.discovery.build(
    api_service_name, api_version, developerKey=DEVELOPER_KEY
)

request = youtube.commentThreads().list(
    part="snippet",
    videoId=video_id,
    maxResults=100
)

comments = []

# Execute the request
response = request.execute()
print("wal3adaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaw")

# Get the comments from the response
for item in response['items']:
    comment = item['snippet']['topLevelComment']['snippet']
    public = item['snippet']['isPublic']
    comments.append([
        comment['authorDisplayName'],
        comment['publishedAt'],
        comment['likeCount'],
        comment['textOriginal'],
        public
    ])

while True:
    try:
        nextPageToken = response['nextPageToken']
    except KeyError:
        break
    nextRequest = youtube.commentThreads().list(
        part="snippet",
        videoId=video_id,
        maxResults=100,
        pageToken=nextPageToken
    )
    response = nextRequest.execute()
    for item in response['items']:
        comment = item['snippet']['topLevelComment']['snippet']
        public = item['snippet']['isPublic']
        comments.append([
            comment['authorDisplayName'],
            comment['publishedAt'],
            comment['likeCount'],
            comment['textOriginal'],
            public
        ])

video_request = youtube.videos().list(
    part="snippet,statistics,contentDetails",
    id=video_id
)
video_response = video_request.execute()

# Extract video metadata
video_title = video_response['items'][0]['snippet']['title']
channel_name = video_response['items'][0]['snippet']['channelTitle']
view_count = video_response['items'][0]['statistics']['viewCount']
like_count = video_response['items'][0]['statistics']['likeCount']

# Extract thumbnail URLs
thumbnails = video_response['items'][0]['snippet']['thumbnails']
default_thumb = thumbnails.get('default', {}).get('url', 'No thumbnail')
medium_thumb = thumbnails.get('medium', {}).get('url', 'No thumbnail')
high_thumb = thumbnails.get('high', {}).get('url', 'No thumbnail')

print(f"Video Title: {video_title}")
print(f"Channel Name: {channel_name}")
print(f"View Count: {view_count}")
print(f"Like Count: {like_count}")
print(f"Thumbnail (default): {default_thumb}")
print(f"Thumbnail (medium): {medium_thumb}")
print(f"Thumbnail (high): {high_thumb}")

# Try to get transcript
try:
    transcript = YouTubeTranscriptApi.get_transcript(video_id)
    transcript_text = " ".join([entry['text'] for entry in transcript])
    print("Transcript retrieved successfully!")
except NoTranscriptFound:
    transcript_text = "No transcript available for this video."
    print("No transcript found.")
except TranscriptsDisabled:
    transcript_text = "Transcripts are disabled for this video."
    print("Transcripts are disabled.")
except Exception as e:
    transcript_text = f"An error occurred: {e}"
    print("An unexpected error occurred:", e)

print("wal3adaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaw")
print(transcript_text)

# Save comments to DataFrame
df = pd.DataFrame(comments, columns=['author', 'updated_at', 'like_count', 'text', 'public'])
df.info()

print("wal3adaaaaaaaaaaaaaaaaaaaaaaaaaaaaw22")
print(transcript_text)

response['items'][0]

df.head(5)

df.sort_values(by='like_count', ascending=False)[0:10]

jdida=df;
#print(jdida)
jdida ['feeling']=None
print(jdida)

# Commented out IPython magic to ensure Python compatibility.
# %pip install -qU langchain-groq

!pip install langchain_groq
from langchain_groq import ChatGroq

api_key = "***"

llm = ChatGroq(
    api_key=api_key,
    model="gemma2-9b-it",
    temperature=0,
    max_tokens=None,
    timeout=None,
    max_retries=2,
)

prompt = f"""
You are a skilled YouTube video analyst.

Based on the video titled "{video_title}" from the channel "{channel_name}" and using the provided transcript below, generate a detailed and insightful report.

The report should include the following:

1. **Video Overview** ‚Äì Summarize the video‚Äôs main themes and key messages in a clear and concise way.
2. **Key Moments and Highlights** ‚Äì Identify important or engaging parts of the video that stand out.
3. **Tone and Sentiment** ‚Äì Describe the tone of the video (e.g., serious, humorous, casual) and the sentiment of the speakers.
4. **Audience and Engagement** ‚Äì Suggest the target audience and explain how the content might appeal to them.

Here is the transcript for analysis:
{transcript_text}
"""

response = llm.invoke(prompt)
print(response.content)

messages = [
    (
        "system",
        "Analyze the sentiment of this comment and respond with only one word: 'positive,' 'very positive' 'negative,' 'very negative,' or 'neutral' based on the tone of the comment.",
    ),
    ("human", "My god this one was too good. Fosh with the vent, Chip with the yes we are, Ethan getting marinated, JJ voting for himself, JJ not doing oxygen, absolute laughs"),
]
ai_msg = llm.invoke(messages)
review=ai_msg.content
print(review)

messages = [
    (
        "system",
        "You are excellent at analyzing sentiment. Based on the comment I provide, respond with a score from 1 to 5, where 1 represents very negative sentiment and 5 represents very positive sentiment. Note I want only the number and that's it",
    ),
    ("human", "My god this one was too good. Fosh with the vent, Chip with the yes we are, Ethan getting marinated, JJ voting for himself, JJ not doing oxygen, absolute laughs"),
]
score_msg = llm.invoke(messages)
score = score_msg.content
print(score)

import pandas as pd
from langchain_groq import ChatGroq
import time

# Initialize the LLM
llm = ChatGroq(
    api_key="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",
    model="gemma2-9b-it",
    temperature=0,
    max_tokens=800,  # Increased to allow more comments per batch
    timeout=30,
    max_retries=3,
)

# Step 1: Select top 900 most liked comments
top_comments = jdida.sort_values(by="like_count", ascending=False).head(900).copy()

# Step 2: Efficient batch analyzer with retry & debug support
def analyze_batch(comments, batch_index):
    prompt = (
        "Analyze the sentiment of the following comments. For each one, respond with ONLY ONE word from this list:\n"
        "'positive', 'very positive', 'negative', 'very negative', 'neutral', 'joyful', 'angry', 'sad', "
        "'questioning', 'confused', 'excited', 'disappointed', 'sarcastic'.\n\n"
    )
    for idx, comment in enumerate(comments):
        prompt += f"{idx + 1}. {comment}\n"
    prompt += "\nReturn ONLY a numbered list of answers, one word per comment."

    for retry in range(3):  # retry on failure
        try:
            print(f"üü° Processing batch {batch_index+1} (attempt {retry+1})...")
            result = llm.invoke(prompt).content.strip()
            lines = result.split("\n")
            responses = [line.split(". ", 1)[1].strip().lower() for line in lines if ". " in line]
            if len(responses) == len(comments):
                print(f"üü¢ Batch {batch_index+1} completed.")
                return responses
            else:
                print(f"üî¥ Batch {batch_index+1}: Mismatch in comment/response count.")
        except Exception as e:
            print(f"üî¥ Batch {batch_index+1} failed: {e}")
            time.sleep(2)
    return ["error"] * len(comments)

# Step 3: Batch processing (batch size increased)
batch_size = 20  # Try 20, adjust if token limit hit
comments = top_comments['text'].tolist()
all_sentiments = []

for i in range(0, len(comments), batch_size):
    batch = comments[i:i + batch_size]
    results = analyze_batch(batch, i // batch_size)
    all_sentiments.extend(results)

# Step 4: Assign results to DataFrame
top_comments['feeling'] = all_sentiments

# Step 5: Check results
print(top_comments['feeling'].value_counts())

import matplotlib.pyplot as plt

# Count the values
sentiment_counts = top_comments['feeling'].value_counts().sort_values(ascending=False)

# Check if there's data to plot
if not sentiment_counts.empty:
    plt.figure(figsize=(10, 6))
    sentiment_counts.plot(
        kind='bar',
        color='skyblue',
        edgecolor='black'
    )

    # Labels and layout
    plt.title('Number of Comments by Sentiment', fontsize=16)
    plt.xlabel('Sentiment', fontsize=14)
    plt.ylabel('Number of Comments', fontsize=14)
    plt.xticks(rotation=45, ha='right', fontsize=12)
    plt.yticks(fontsize=12)
    plt.grid(axis='y', linestyle='--', alpha=0.6)
    plt.tight_layout()
    plt.show()
else:
    print("‚ö†Ô∏è No sentiment data available to plot.")

# Always print the counts (for debug or info)
print(sentiment_counts)

import matplotlib.pyplot as plt

# Sort the DataFrame by 'like_count' in descending order and select the top 100
#top_100_comments = jdida.sort_values(by='like_count', ascending=False).head(100)

# Group by the 'feeling' column and count the occurrences
top_100_sentiment_counts = top_comments['feeling'].value_counts()

# Plotting the data
plt.figure(figsize=(8, 6))
top_100_sentiment_counts.plot(kind='bar', color='purple', edgecolor='black')

# Adding labels and title
plt.title('Sentiment Distribution of Top 100 Liked Comments', fontsize=14)
plt.xlabel('Sentiment', fontsize=12)
plt.ylabel('Number of Comments', fontsize=12)
plt.xticks(rotation=45)
plt.grid(axis='y', linestyle='--', alpha=0.7)

# Show the plot
plt.tight_layout()
plt.show()

# Print the counts for reference
print(top_100_sentiment_counts)
#print(report)
qlawi = [
    (
        "system",
        f"You are a skilled YouTube video analyst. Using the video report'{response}' tell us what you think of the video based on the feedback"

    ),
    (
        "human",
        f"Here is the top 900 liked comments: '{top_100_sentiment_counts}'"
    ),
]
sent_count_msg = llm.invoke(qlawi)
reportt=sent_count_msg.content
print(reportt)

# Step 6: Sentiment scoring and metrics
sentiment_weights = {
    'very positive': 1.0,
    'positive': 0.8,
    'joyful': 0.9,
    'excited': 0.9,
    'neutral': 0.5,
    'confused': 0.3,
    'questioning': 0.3,
    'disappointed': 0.2,
    'negative': 0.2,
    'angry': 0.1,
    'sad': 0.1,
    'very negative': 0.0,
    'sarcastic': 0.1,
    'error': 0.0
}

# Map scores
top_comments['sentiment_score'] = top_comments['feeling'].map(sentiment_weights)

# Score out of 10
avg_score = top_comments['sentiment_score'].mean()
score_out_of_10 = round(avg_score * 10, 2)

# Positive %
positive_labels = ['positive', 'very positive', 'joyful', 'excited']
positive_count = top_comments['feeling'].isin(positive_labels).sum()
total_valid = top_comments['feeling'].isin(sentiment_weights.keys()).sum()
positive_percentage = round((positive_count / total_valid) * 100, 2)

# Engagement rate (comments-to-likes)
total_comments = len(top_comments)
total_likes = top_comments['like_count'].sum()
engagement_rate = round((total_comments / total_likes) * 100, 2)

# Final report
print("\nüìä Sentiment Summary:")
print(f"‚û°Ô∏è Sentiment Score (/10): {score_out_of_10}")
print(f"‚úÖ Positive Comments: {positive_percentage}%")
print(f"üìà Engagement Rate: {engagement_rate}%")

import matplotlib.pyplot as plt

# Define high-level sentiment groups
positive_labels = ['very positive', 'positive', 'joyful', 'excited']
neutral_labels = ['neutral', 'confused', 'questioning', 'sarcastic']
negative_labels = ['very negative', 'negative', 'disappointed', 'angry', 'sad']

# Count the number of comments in each group
category_counts = {
    'Positive': top_comments['feeling'].isin(positive_labels).sum(),
    'Neutral': top_comments['feeling'].isin(neutral_labels).sum(),
    'Negative': top_comments['feeling'].isin(negative_labels).sum()
}

# --- Pie Chart ---
plt.figure(figsize=(7, 6))
plt.pie(
    category_counts.values(),
    labels=category_counts.keys(),
    autopct='%1.1f%%',
    startangle=140,
    colors=['#66bb6a', '#ffa726', '#ef5350']
)
plt.title('Sentiment Breakdown (Positive / Neutral / Negative)')
plt.axis('equal')
plt.tight_layout()
plt.show()

# --- Horizontal Bar Chart ---
plt.figure(figsize=(8, 5))
plt.barh(
    list(category_counts.keys()),
    list(category_counts.values()),
    color=['#66bb6a', '#ffa726', '#ef5350'],
    edgecolor='black'
)
plt.title('Sentiment Distribution (Top 900 Comments)')
plt.xlabel('Number of Comments')
plt.ylabel('Sentiment Category')
plt.grid(axis='x', linestyle='--', alpha=0.6)
plt.tight_layout()
plt.show()

!pip install wordcloud
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import nltk
from nltk.corpus import stopwords

# Download stopwords if not already
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

# Join all comments into one big string
text = " ".join(top_comments['text'].astype(str).tolist())

# Create WordCloud object
wordcloud = WordCloud(
    width=800,
    height=400,
    background_color='white',
    stopwords=stop_words,
    max_words=200,
    colormap='viridis'
).generate(text)

# Display the generated image
plt.figure(figsize=(10, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title("Most Frequent Words in Comments", fontsize=14)
plt.tight_layout()
plt.show()

from langchain_groq import ChatGroq

llm = ChatGroq(
    api_key="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",
    model="gemma2-9b-it",
    temperature=0,
    max_tokens=600,
    timeout=30,
    max_retries=3,
)

def run_prompt(prompt_template, comments):
    comments_text = "\n".join(f"{i+1}. {c}" for i, c in enumerate(comments))
    prompt = prompt_template.format(comments_list=comments_text)
    messages = [
        ("system", "You are a skilled YouTube video analyst."),
        ("human", prompt)
    ]
    response = llm.invoke(messages)
    return response.content.strip()

# Prepare top N comments text (e.g., top 20 by likes)
top_comments_sample = top_comments.sort_values(by="like_count", ascending=False).head(20)['text'].tolist()

key_insights_prompt = """
Analyze the following YouTube comments and provide 4 concise key insights about the audience, tone, and engagement.
Focus on what these comments reveal about:
- Educational value
- Technical content level
- Audience type (beginner, expert, etc.)
- Overall sentiment trends and engagement signals

Comments:
{comments_list}

Provide your response as a bullet point list.
"""

recommendations_prompt = """
Based on the following YouTube comments and their general sentiment, write 4 actionable recommendations for content creators to improve engagement and audience satisfaction.
Focus on:
- Content strategy
- Community engagement
- Follow-up content ideas
- Posting and optimization tips

Comments:
{comments_list}

Provide your response as a bullet point list.
"""

key_insights = run_prompt(key_insights_prompt, top_comments_sample)
recommendations = run_prompt(recommendations_prompt, top_comments_sample)

print("=== Key Insights ===")
print(key_insights)
print("\n=== Recommendations ===")
print(recommendations)